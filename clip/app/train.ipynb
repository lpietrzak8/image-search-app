{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoImageProcessor,\n    CLIPModel,\n    TrainingArguments,\n    Trainer,\n    set_seed,\n)\nfrom peft import LoraConfig, get_peft_model\nfrom torchvision.transforms import (\n    Resize, CenterCrop, ToTensor, Normalize, Compose, \n    InterpolationMode, RandomResizedCrop\n)\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:23:22.952070Z","iopub.execute_input":"2026-01-07T21:23:22.952857Z","iopub.status.idle":"2026-01-07T21:23:22.958776Z","shell.execute_reply.started":"2026-01-07T21:23:22.952824Z","shell.execute_reply":"2026-01-07T21:23:22.958058Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"EXPERIMENT_MODE = \"full\" \n\nDATASET_NAME = \"AnyModal/flickr30k\"\nIMAGE_COLUMN = \"image\"\nCAPTION_COLUMN = \"original_alt_text\"\nMODEL_NAME = \"openai/clip-vit-base-patch32\"\nOUTPUT_DIR = \"./clip-flickr30k-lora\"\nSEED = 42\n\nCONFIGS = {\n    \"fast\": {\n        \"desc\": \"Quick test\",\n        \"use_subset\": True,\n        \"subset_size\": 10000,\n        \"batch_size\": 64,\n        \"grad_accum\": 1,\n        \"lr\": 5e-5,\n        \"epochs\": 1,\n        \"lora_r\": 16,\n        \"lora_modules\": [\"q_proj\", \"v_proj\"],\n        \"save_steps\": 100,\n        \"eval_steps\": 100,\n    },\n    \"full\": {\n        \"desc\": \"Best quality\",\n        \"use_subset\": False,\n        \"subset_size\": None,\n        \"batch_size\": 32,\n        \"grad_accum\": 2,\n        \"lr\": 1e-4,\n        \"epochs\": 3,\n        \"lora_r\": 16,\n        \"lora_modules\": [\"q_proj\", \"v_proj\"],\n        \"save_steps\": 100,\n        \"eval_steps\": 100,\n    }\n}\n\nconfig = CONFIGS[EXPERIMENT_MODE]\nprint(f\"\\n{'='*60}\")\nprint(f\"MODE: {EXPERIMENT_MODE.upper()}\")\nprint(f\"Description: {config['desc']}\")\nprint(f\"{'='*60}\\n\")\n\n\nUSE_SUBSET = config[\"use_subset\"]\nSUBSET_SIZE = config[\"subset_size\"]\nPER_DEVICE_TRAIN_BATCH_SIZE = config[\"batch_size\"]\nPER_DEVICE_EVAL_BATCH_SIZE = config[\"batch_size\"]\nGRADIENT_ACCUMULATION_STEPS = config[\"grad_accum\"]\nLEARNING_RATE = config[\"lr\"]\nNUM_TRAIN_EPOCHS = config[\"epochs\"]\nLORA_R = config[\"lora_r\"]\nLORA_TARGET_MODULES = config[\"lora_modules\"]\nSAVE_STEPS = config[\"save_steps\"]\nEVAL_STEPS = config[\"eval_steps\"]\n\n\nWEIGHT_DECAY = 0.01\nWARMUP_STEPS = 200\nMAX_SEQ_LENGTH = 77\nLOGGING_STEPS = 25\nLORA_ALPHA = LORA_R * 2\nLORA_DROPOUT = 0.2\n\nprint(f\"Config Summary:\")\nprint(f\"  Batch Size: {PER_DEVICE_TRAIN_BATCH_SIZE} (effective: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  LoRA Rank: {LORA_R}\")\nprint(f\"  LoRA Modules: {LORA_TARGET_MODULES}\")\nprint(f\"  Epochs: {NUM_TRAIN_EPOCHS}\")\nif USE_SUBSET:\n    print(f\"  Dataset Size: {SUBSET_SIZE} examples (subset)\")\nelse:\n    print(f\"  Dataset Size: Full dataset\")\nprint()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:23:30.077094Z","iopub.execute_input":"2026-01-07T21:23:30.077846Z","iopub.status.idle":"2026-01-07T21:23:30.087842Z","shell.execute_reply.started":"2026-01-07T21:23:30.077819Z","shell.execute_reply":"2026-01-07T21:23:30.087145Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nMODE: FULL\nDescription: Best quality - 5-7 hours on T4 GPU\n============================================================\n\nConfig Summary:\n  Batch Size: 32 (effective: 64)\n  Learning Rate: 0.0001\n  LoRA Rank: 16\n  LoRA Modules: ['q_proj', 'v_proj']\n  Epochs: 3\n  Dataset Size: Full dataset\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"set_seed(SEED)\n\ndataset = load_dataset(DATASET_NAME)\nprint(\"Original dataset:\", dataset)\n\nif USE_SUBSET and \"train\" in dataset:\n    print(f\"\\nUsing subset: {SUBSET_SIZE} training examples\")\n    dataset[\"train\"] = dataset[\"train\"].select(range(min(SUBSET_SIZE, len(dataset[\"train\"]))))\n   \n    val_size = min(int(SUBSET_SIZE * 0.1), len(dataset[\"validation\"]))\n    dataset[\"validation\"] = dataset[\"validation\"].select(range(val_size))\n\nprint(f\"\\nFinal dataset sizes:\")\nprint(f\"  Train: {len(dataset['train'])} examples\")\nprint(f\"  Validation: {len(dataset['validation'])} examples\")\nprint(f\"  Test: {len(dataset['test'])} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:23:37.700241Z","iopub.execute_input":"2026-01-07T21:23:37.700570Z","iopub.status.idle":"2026-01-07T21:23:38.664185Z","shell.execute_reply.started":"2026-01-07T21:23:37.700543Z","shell.execute_reply":"2026-01-07T21:23:38.663361Z"}},"outputs":[{"name":"stdout","text":"Original dataset: DatasetDict({\n    train: Dataset({\n        features: ['image', 'alt_text', 'sentids', 'split', 'img_id', 'filename', 'original_alt_text'],\n        num_rows: 29000\n    })\n    validation: Dataset({\n        features: ['image', 'alt_text', 'sentids', 'split', 'img_id', 'filename', 'original_alt_text'],\n        num_rows: 1014\n    })\n    test: Dataset({\n        features: ['image', 'alt_text', 'sentids', 'split', 'img_id', 'filename', 'original_alt_text'],\n        num_rows: 1000\n    })\n})\n\nFinal dataset sizes:\n  Train: 29000 examples\n  Validation: 1014 examples\n  Test: 1000 examples\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nimage_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n\nimage_size = image_processor.size.get(\"shortest_edge\", 224)\nimage_mean = image_processor.image_mean\nimage_std = image_processor.image_std\n\nprint(f\"Image size: {image_size}, Mean: {image_mean}, Std: {image_std}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:23:45.291635Z","iopub.execute_input":"2026-01-07T21:23:45.292253Z","iopub.status.idle":"2026-01-07T21:23:45.793581Z","shell.execute_reply.started":"2026-01-07T21:23:45.292221Z","shell.execute_reply":"2026-01-07T21:23:45.792861Z"}},"outputs":[{"name":"stdout","text":"Image size: 224, Mean: [0.48145466, 0.4578275, 0.40821073], Std: [0.26862954, 0.26130258, 0.27577711]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"train_transform = Compose([\n    Resize(int(image_size * 1.15), interpolation=InterpolationMode.BICUBIC),\n    RandomResizedCrop(image_size, scale=(0.8, 1.0), interpolation=InterpolationMode.BICUBIC),\n    ToTensor(),\n    Normalize(mean=image_mean, std=image_std),\n])\n\neval_transform = Compose([\n    Resize(int(image_size * 1.15), interpolation=InterpolationMode.BICUBIC),\n    CenterCrop(image_size),\n    ToTensor(),\n    Normalize(mean=image_mean, std=image_std),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:23:48.105605Z","iopub.execute_input":"2026-01-07T21:23:48.106272Z","iopub.status.idle":"2026-01-07T21:23:48.111949Z","shell.execute_reply.started":"2026-01-07T21:23:48.106241Z","shell.execute_reply":"2026-01-07T21:23:48.111256Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\nimport random\ndef transform_train_combined(examples):\n    images = examples[IMAGE_COLUMN]\n    captions = examples[CAPTION_COLUMN]\n    \n    pixel_values = []\n    for img in images:\n        if isinstance(img, str):\n            img = Image.open(img).convert(\"RGB\")\n        elif isinstance(img, dict) and \"path\" in img:\n            img = Image.open(img[\"path\"]).convert(\"RGB\")\n        pixel_values.append(train_transform(img))\n    \n    \n    caption_texts = []\n    for c in captions:\n        if isinstance(c, list) and len(c) > 0:\n            caption_texts.append(random.choice(c))\n        else:\n            caption_texts.append(c if isinstance(c, str) else c[0])\n    \n   \n    tokens = tokenizer(\n        caption_texts,\n        max_length=MAX_SEQ_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": tokens[\"input_ids\"],\n        \"attention_mask\": tokens[\"attention_mask\"]\n    }\n\ndef transform_eval_combined(examples):\n   \n    images = examples[IMAGE_COLUMN]\n    captions = examples[CAPTION_COLUMN]\n\n    pixel_values = []\n    for img in images:\n        if isinstance(img, str):\n            img = Image.open(img).convert(\"RGB\")\n        elif isinstance(img, dict) and \"path\" in img:\n            img = Image.open(img[\"path\"]).convert(\"RGB\")\n        pixel_values.append(eval_transform(img))\n    \n    caption_texts = [c[0] if isinstance(c, list) else c for c in captions]\n    \n    tokens = tokenizer(\n        caption_texts,\n        max_length=MAX_SEQ_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": tokens[\"input_ids\"],\n        \"attention_mask\": tokens[\"attention_mask\"]\n    }\n\n\ndataset[\"train\"].set_transform(transform_train_combined)\ndataset[\"validation\"].set_transform(transform_eval_combined)\n\nprint(\"\\nCombined transforms configured:\")\nprint(\"  Training: RANDOM caption per epoch\")\nprint(\"  Validation: First caption only\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:24:02.967281Z","iopub.execute_input":"2026-01-07T21:24:02.967949Z","iopub.status.idle":"2026-01-07T21:24:03.035501Z","shell.execute_reply.started":"2026-01-07T21:24:02.967922Z","shell.execute_reply":"2026-01-07T21:24:03.034865Z"}},"outputs":[{"name":"stdout","text":"\nâœ“ Combined transforms configured:\n  Training: RANDOM caption per epoch (data augmentation!)\n  Validation: First caption only (deterministic)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def collate_fn(examples):\n    return {\n        \"pixel_values\": torch.stack([ex[\"pixel_values\"] for ex in examples]),\n        \"input_ids\": torch.tensor([ex[\"input_ids\"] for ex in examples], dtype=torch.long),\n        \"attention_mask\": torch.tensor([ex[\"attention_mask\"] for ex in examples], dtype=torch.long),\n        \"return_loss\": True\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:24:16.232402Z","iopub.execute_input":"2026-01-07T21:24:16.233137Z","iopub.status.idle":"2026-01-07T21:24:16.237593Z","shell.execute_reply.started":"2026-01-07T21:24:16.233106Z","shell.execute_reply":"2026-01-07T21:24:16.236863Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(\"\\nLoading model...\")\nbase_model = CLIPModel.from_pretrained(MODEL_NAME)\nbase_model.to(device)\n\n\nfor p in base_model.parameters():\n    p.requires_grad = False\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=LORA_TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n)\n\nmodel = get_peft_model(base_model, lora_config)\nmodel.to(device)\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"\\nModel loaded!\")\nprint(f\"Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\nprint(f\"Total params: {total:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:24:19.785522Z","iopub.execute_input":"2026-01-07T21:24:19.786217Z","iopub.status.idle":"2026-01-07T21:24:20.862454Z","shell.execute_reply.started":"2026-01-07T21:24:19.786190Z","shell.execute_reply":"2026-01-07T21:24:20.861762Z"}},"outputs":[{"name":"stdout","text":"\nLoading model...\n\nModel loaded!\nTrainable params: 983,040 (0.65%)\nTotal params: 152,260,353\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import EarlyStoppingCallback\n\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n    warmup_steps=WARMUP_STEPS,\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    \n    \n    logging_steps=LOGGING_STEPS,\n    logging_first_step=True,\n    \n    save_steps=SAVE_STEPS,\n    eval_strategy=\"steps\",\n    eval_steps=EVAL_STEPS,\n    save_total_limit=2,\n    \n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    fp16=True,  \n    dataloader_num_workers=4, \n    dataloader_pin_memory=True,\n    \n    report_to=\"none\",\n    remove_unused_columns=False,\n    push_to_hub=False,\n\n    logging_strategy=\"steps\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    data_collator=collate_fn,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\nprint(\"\\nTrainer configured\")\nprint(f\"Total training steps: {len(dataset['train']) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS}\")\nprint(f\"Logging every {LOGGING_STEPS} steps\")\nprint(f\"Evaluating every {EVAL_STEPS} steps\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:42:12.247916Z","iopub.execute_input":"2026-01-07T21:42:12.248310Z","iopub.status.idle":"2026-01-07T21:42:12.310563Z","shell.execute_reply.started":"2026-01-07T21:42:12.248240Z","shell.execute_reply":"2026-01-07T21:42:12.310008Z"}},"outputs":[{"name":"stdout","text":"\nTrainer configured!\nTotal training steps: 1359\nLogging every 25 steps\nEvaluating every 100 steps\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60 + \"\\n\")\n\ntrain_result = trainer.train()\n\n\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nimage_processor.save_pretrained(OUTPUT_DIR)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\nTraining metrics:\")\nfor key, value in train_result.metrics.items():\n    print(f\"  {key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:42:17.708244Z","iopub.execute_input":"2026-01-07T21:42:17.709048Z","iopub.status.idle":"2026-01-07T21:42:20.533078Z","shell.execute_reply.started":"2026-01-07T21:42:17.709017Z","shell.execute_reply":"2026-01-07T21:42:20.527290Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSTARTING TRAINING\n============================================================\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/462287593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4142\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   4143\u001b[0m                     \u001b[0;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4144\u001b[0m                     \u001b[0;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits_per_image,logits_per_text,text_embeds,image_embeds,text_model_output,vision_model_output. For reference, the inputs it received are pixel_values,input_ids,attention_mask."],"ename":"ValueError","evalue":"The model did not return a loss from the inputs, only the following keys: logits_per_image,logits_per_text,text_embeds,image_embeds,text_model_output,vision_model_output. For reference, the inputs it received are pixel_values,input_ids,attention_mask.","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"print(\"\\nRunning final evaluation\")\neval_metrics = trainer.evaluate()\n\nprint(\"\\nFinal validation metrics:\")\nfor key, value in eval_metrics.items():\n    print(f\"  {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T21:37:29.488825Z","iopub.execute_input":"2026-01-07T21:37:29.489173Z","iopub.status.idle":"2026-01-07T21:37:35.546161Z","shell.execute_reply.started":"2026-01-07T21:37:29.489144Z","shell.execute_reply":"2026-01-07T21:37:35.545372Z"}},"outputs":[{"name":"stdout","text":"\nRunning final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16/16 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nFinal validation metrics:\n  eval_loss: 0.11009262502193451\n  eval_runtime: 6.0484\n  eval_samples_per_second: 167.647\n  eval_steps_per_second: 2.645\n  epoch: 3.0\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from pathlib import Path\nPath(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\nmodel.save_pretrained(OUTPUT_DIR)\nprint(f\"\\nLoRA adapters saved to: {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T13:36:51.141370Z","iopub.status.idle":"2026-01-07T13:36:51.141609Z","shell.execute_reply.started":"2026-01-07T13:36:51.141496Z","shell.execute_reply":"2026-01-07T13:36:51.141510Z"}},"outputs":[],"execution_count":null}]}