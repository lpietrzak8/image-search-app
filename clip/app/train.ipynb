{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831d1a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor,\n",
    "    CLIPModel,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Resize, CenterCrop, ToTensor, Normalize, Compose, InterpolationMode\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ff09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EDIT THESE \n",
    "DATASET_NAME = \"arampacha/rsicd\"        # None if using local files\n",
    "LOCAL_TRAIN_FILE = None                # \"train.json\" or \"train.csv\" if not using HF dataset\n",
    "LOCAL_VALID_FILE = None                # optional\n",
    "IMAGE_COLUMN = \"image\"                 # adjust to dataset's column name (e.g., \"image\" or \"image_path\")\n",
    "CAPTION_COLUMN = \"captions\"            # adjust to dataset's caption column name\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "OUTPUT_DIR = \"./clip-lora-output\"\n",
    "SEED = 42\n",
    "\n",
    "# Training hyperparams\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 77   # CLIP default\n",
    "SAVE_STEPS = 500\n",
    "LOGGING_STEPS = 100\n",
    "\n",
    "# LoRA hyperparams (sane defaults)\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 128\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\"]  # typical for HF CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b8d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_module_names(model, prefix=\"\"):\n",
    "    names = []\n",
    "    for name, module in model.named_modules():\n",
    "        names.append(name)\n",
    "    return names\n",
    "\n",
    "\n",
    "# model_tmp = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "# print(\"\\n\".join([n for n in list_module_names(model_tmp) if \"attn\" in n or \"q_proj\" in n or \"proj\" in n][:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c85d88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████████████████████████████████████████████████████| 8734/8734 [00:00<00:00, 10262.29 examples/s]\n",
      "Generating test split: 100%|█████████████████████████████████████████████████████| 1093/1093 [00:00<00:00, 10044.31 examples/s]\n",
      "Generating valid split: 100%|█████████████████████████████████████████████████████| 1094/1094 [00:00<00:00, 6104.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 8734\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1093\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "})\n",
      "im changing it\n",
      "Dataset({\n",
      "    features: ['filename', 'captions', 'image'],\n",
      "    num_rows: 1094\n",
      "})\n",
      "Loaded local dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 8734\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1093\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "if DATASET_NAME is not None:\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    print(\"Loaded HF dataset:\", dataset)\n",
    "else:\n",
    "    # Expect CSV or JSON files\n",
    "    data_files = {}\n",
    "    if LOCAL_TRAIN_FILE:\n",
    "        data_files[\"train\"] = LOCAL_TRAIN_FILE\n",
    "    if LOCAL_VALID_FILE:\n",
    "        data_files[\"validation\"] = LOCAL_VALID_FILE\n",
    "    assert data_files, \"Either set DATASET_NAME or provide local train/validation files.\"\n",
    "    ext = LOCAL_TRAIN_FILE.split(\".\")[-1]\n",
    "    dataset = load_dataset(ext, data_files=data_files)\n",
    "    print(\"Loaded local dataset:\", dataset)\n",
    "\n",
    "if \"valid\" in dataset and \"validation\" not in dataset:\n",
    "    print(\"im changing it\")\n",
    "    dataset[\"validation\"] = dataset[\"valid\"]\n",
    "    print(dataset[\"validation\"])\n",
    "\n",
    "\n",
    "# if \"train\" in dataset:\n",
    "#     dataset[\"train\"] = dataset[\"train\"].select(range(min(16, len(dataset[\"train\"]))))\n",
    "    \n",
    "\n",
    "# if \"validation\" in dataset:\n",
    "#     dataset[\"validation\"] = dataset[\"validation\"].select(range(min(16, len(dataset[\"validation\"]))))\n",
    "   \n",
    "# def preprocess(example):\n",
    "#     # image → pixel_values\n",
    "#     image = example[\"image\"]\n",
    "#     example[\"pixel_values\"] = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n",
    "\n",
    "#     # caption → input_ids + attention_mask\n",
    "#     text = example[\"captions\"][0] if isinstance(example[\"captions\"], list) else example[\"captions\"]\n",
    "#     text_tokens = tokenizer(text, truncation=True, padding=\"max_length\", max_length=77)\n",
    "#     example[\"input_ids\"] = text_tokens[\"input_ids\"]\n",
    "#     example[\"attention_mask\"] = text_tokens[\"attention_mask\"]\n",
    "#         return example\n",
    "\n",
    "print(\"Loaded local dataset:\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51be118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_size: 224 mean/std: [0.48145466, 0.4578275, 0.40821073] [0.26862954, 0.26130258, 0.27577711]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# normalization values from processor to match pretrained CLIP\n",
    "image_size = image_processor.size[\"shortest_edge\"] if isinstance(image_processor.size, dict) else getattr(image_processor, \"image_mean\", 224)\n",
    "image_mean = image_processor.image_mean\n",
    "image_std = image_processor.image_std\n",
    "\n",
    "print(\"image_size:\", image_size, \"mean/std:\", image_mean, image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f339a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop\n",
    "\n",
    "train_transform = Compose([\n",
    "    Resize(int(image_size * 1.15), interpolation=InterpolationMode.BICUBIC),\n",
    "    RandomResizedCrop(int(image_size), scale=(0.8, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=image_mean, std=image_std),\n",
    "])\n",
    "\n",
    "eval_transform = Compose([\n",
    "    Resize(int(image_size * 1.15), interpolation=InterpolationMode.BICUBIC),\n",
    "    CenterCrop(int(image_size)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=image_mean, std=image_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(examples):\n",
    "    # captions might be list-of-lists depending on dataset; unify to string\n",
    "    caps = [c[0] if isinstance(c, list) else c for c in examples[CAPTION_COLUMN]]\n",
    "    tokenized = tokenizer(caps, max_length=MAX_SEQ_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = tokenized[\"input_ids\"]\n",
    "    examples[\"attention_mask\"] = tokenized[\"attention_mask\"]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def transform_images(examples, is_train=True):\n",
    "    imgs = examples[IMAGE_COLUMN]\n",
    "    out = []\n",
    "    transform = train_transform if is_train else eval_transform\n",
    "    for im in imgs:\n",
    "        if isinstance(im, str):\n",
    "            im = Image.open(im).convert(\"RGB\")\n",
    "        elif isinstance(im, dict) and \"path\" in im: \n",
    "            im = Image.open(im[\"path\"]).convert(\"RGB\")\n",
    "        # else assume PIL.Image\n",
    "        out.append(transform(im))\n",
    "    examples[\"pixel_values\"] = out\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ef7baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 8734\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1093\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 8734/8734 [00:00<00:00, 11051.32 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████| 1094/1094 [00:00<00:00, 8223.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validation ready.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['captions', 'image', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 8734\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1093\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['captions', 'image', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "if \"train\" in dataset:\n",
    "    dataset[\"train\"] = dataset[\"train\"].map(tokenize_captions, batched=True, remove_columns=[c for c in dataset[\"train\"].column_names if c not in [IMAGE_COLUMN, CAPTION_COLUMN]])\n",
    "    dataset[\"train\"].set_transform(lambda x: transform_images(x, is_train=True))\n",
    "if \"validation\" in dataset or \"valid\" in dataset:\n",
    "    if \"valid\" in dataset and \"validation\" not in dataset:\n",
    "        dataset[\"validation\"] = dataset.pop(\"valid\")\n",
    "    dataset[\"validation\"] = dataset[\"validation\"].map(tokenize_captions, batched=True, remove_columns=[c for c in dataset[\"validation\"].column_names if c not in [IMAGE_COLUMN, CAPTION_COLUMN]])\n",
    "    dataset[\"validation\"].set_transform(lambda x: transform_images(x, is_train=False))\n",
    "\n",
    "print(\"Train/Validation ready.\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d732b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([ex[\"pixel_values\"] for ex in examples])\n",
    "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in examples], dtype=torch.long)\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"return_loss\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8856a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 5898240 (3.7526% of total)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import PeftConfig\n",
    "\n",
    "# base CLIP\n",
    "base_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)\n",
    "\n",
    "# Freeze original params\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "#     task_type=\"FEATURE_EXTRACTION\",\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def print_trainable(m):\n",
    "    t = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    print(f\"Trainable params: {t} ({t/total:.4%} of total)\")\n",
    "\n",
    "print_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7cb356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['captions', 'image', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 8734\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1093\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filename', 'captions', 'image'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['captions', 'image', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1094\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 8734/8734 [01:16<00:00, 114.70 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 1093/1093 [00:08<00:00, 124.92 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 1094/1094 [00:09<00:00, 120.08 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 1094/1094 [00:09<00:00, 109.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['captions', 'image', 'input_ids', 'attention_mask', 'pixel_values'])\n",
      "Dataset({\n",
      "    features: ['captions', 'image', 'input_ids', 'attention_mask', 'pixel_values'],\n",
      "    num_rows: 8734\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(example):\n",
    "    # image → pixel_values\n",
    "    image = example[\"image\"]\n",
    "    # Keep tensor, remove [0]\n",
    "    example[\"pixel_values\"] = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "    # caption → input_ids + attention_mask\n",
    "    text = example[\"captions\"][0] if isinstance(example[\"captions\"], list) else example[\"captions\"]\n",
    "    text_tokens = tokenizer(text, truncation=True, padding=\"max_length\", max_length=77)\n",
    "    example[\"input_ids\"] = torch.tensor(text_tokens[\"input_ids\"], dtype=torch.long)\n",
    "    example[\"attention_mask\"] = torch.tensor(text_tokens[\"attention_mask\"], dtype=torch.long)\n",
    "\n",
    "    return example\n",
    "\n",
    "    \n",
    "print(dataset)\n",
    "dataset = dataset.map(preprocess, batched=False, load_from_cache_file=False)\n",
    "print(dataset[\"train\"][0].keys())\n",
    "\n",
    "# from transformers import Trainer\n",
    "\n",
    "# class MyTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # extract only the keys your CLIPModel accepts\n",
    "#         pixel_values = inputs.pop(\"pixel_values\")\n",
    "#         input_ids = inputs.pop(\"input_ids\")\n",
    "#         attention_mask = inputs.pop(\"attention_mask\")\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             pixel_values=pixel_values,\n",
    "#             return_dict=True\n",
    "#         )\n",
    "#         loss = outputs.loss\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    eval_strategy=\"steps\" if \"validation\" in dataset else \"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"] if \"train\" in dataset else None,\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b265ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3276' max='3276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3276/3276 1:11:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>2.526394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>2.479175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>2.435039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>2.359408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>2.392176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>2.419033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>2.309820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>2.368917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>2.493088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.318300</td>\n",
       "      <td>2.426887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>2.430271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>2.453510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>2.553012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>2.484341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>2.496924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>2.511965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>2.504684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>2.483153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>2.517879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>2.546249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>2.481930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>2.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>2.499228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>2.487578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>2.542503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>2.550372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>2.534537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.178800</td>\n",
       "      <td>2.567919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>2.588920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>2.589918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.170300</td>\n",
       "      <td>2.603474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>2.605122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Metrics:\n",
      "{'train_runtime': 4268.1647, 'train_samples_per_second': 6.139, 'train_steps_per_second': 0.768, 'total_flos': 1595491786934892.0, 'train_loss': 0.28273216621343034, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasz/Documents/ug/III_rok/image-search/clip/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='137' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [137/137 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 2.6055562496185303, 'eval_runtime': 63.5454, 'eval_samples_per_second': 17.216, 'eval_steps_per_second': 2.156, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_result = None\n",
    "if \"train\" in dataset:\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model(OUTPUT_DIR) \n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    image_processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    print(\"Training finished. Metrics:\")\n",
    "    print(train_result.metrics)\n",
    "\n",
    "\n",
    "if \"validation\" in dataset:\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5ffed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapters to: ./clip-lora-output\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved LoRA adapters to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f85b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "# load base\n",
    "base = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "# load adapter onto base \n",
    "peft = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "peft.to(device)\n",
    "proc = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "img = Image.open(\"path_to_a_test_image.jpg\").convert(\"RGB\") \n",
    "inputs = proc(text=[\"a cat\", \"a dog\"], images=img, return_tensors=\"pt\", padding=True).to(device)\n",
    "out = peft(**inputs)\n",
    "print(\"Logits per image:\", out.logits_per_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clip)",
   "language": "python",
   "name": "clip-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
